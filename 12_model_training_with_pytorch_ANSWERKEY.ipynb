{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training in PyTorch\n",
        "\n",
        "In this lab, you will explore the core concepts and steps involved in ***building a convolutional neural network (CNN) using PyTorch***, a leading deep learning framework. CNNs are particularly well-suited for image classification tasks due to their ability to automatically learn spatial hierarchies in images.\n",
        "\n",
        "**Throughout the lab, you will:**\n",
        "\n",
        "* Define a simple CNN architecture using torch.nn.Module.\n",
        "* Work with image data by loading and transforming it for training.\n",
        "* Implement the forward pass with convolutional, pooling, and fully connected layers.\n",
        "* Optimize the model using an optimizer like SGD or Adam.\n",
        "* Evaluate the model's performance with metrics such as loss and accuracy.\n",
        "\n",
        "By the end of the lab, you will have a foundational understanding of CNNs in PyTorch and how to train them for image classification tasks. This hands-on experience will serve as a stepping stone for building more advanced, custom CNN architectures.\n",
        "\n",
        "As was practice in previous labs, `XXXX` means you have to fill in the correct code. If you are following along and not in our course at the University of Rhode Island, you can find the answers in the `12-model-training-with-pytorch-ANSWERKEY.ipynb` file in the repository.\n",
        "\n",
        "Let’s begin by importing the necessary libraries for your first CNN.\n",
        "\n",
        "## 1. Set up"
      ],
      "metadata": {
        "id": "kpU_VSkO8g3H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RoXxDsjobaa1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# for resnet\n",
        "import torchvision.models as models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Parameters for Convolutional Layers\n",
        "\n",
        "There a few parameters that we have to specify for each convolutional layer that we add. Below is a description of what they are and how to select appropriate values.\n",
        "\n",
        "#### **`in_channels`: The Number of Input Feature Maps**\n",
        "\n",
        "* `in_channels` refers to the number of input channels (feature maps) being fed into a convolutional layer.\n",
        "\n",
        "* It must match the number of output channels from the previous layer (except for the first layer, which depends on the input data).\n",
        "\n",
        "  **First Convolutional Layer**:\n",
        "\n",
        "  * If the input is an RGB image (CIFAR-10, ImageNet, etc.), it has 3 channels (R, G, B), so `in_channels`=3.\n",
        "\n",
        "  * If the input is a grayscale image (MNIST, medical imaging, etc.), it has 1 channel, so `in_channels`=1.\n",
        "\n",
        "  **Subsequent Layers**:\n",
        "\n",
        "  * The `in_channels` for each layer is equal to the out_channels of the previous convolutional layer.\n",
        "\n",
        "#### **`out_channels`: The Number of Output Feature Maps**\n",
        "\n",
        "* `out_channels` determines how many feature maps (filters) the convolutional layer will output.\n",
        "\n",
        "* Each filter in a CNN learns to detect different features (edges, textures, shapes, etc.), so increasing `out_channels` allows the model to learn more complex patterns.\n",
        "\n",
        "  **Typical Design Choices:**\n",
        "\n",
        "  * Start with a small number of filters (e.g., `out_channels`=16 or\n",
        "`out_channels`=32) to extract low-level patterns.\n",
        "\n",
        "  * Gradually increase `out_channels` (e.g., 32 → 64 → 128 → 256) as the network goes deeper, capturing more abstract features.\n",
        "\n",
        "#### **`kernel_size`: The x,y dimensions of the filters**\n",
        "  \n",
        "  * Start with a smaller filter (e.g., `kernel_size`=3) and work your way to larger filters if needed.\n",
        "\n",
        "#### **`padding`: The additional pixels added around your image**\n",
        "  \n",
        "  * Typically set to zero.\n",
        "  * Add padding if the information at the edges of your images is highly important to your task.\n",
        "  \n",
        "#### **`stride`: The number of pixels we shift our kernel**\n",
        "\n",
        "* `stride` refers to the size of the shift of the kernel across the image at each step.\n",
        "* Practitioners oven opt for a `stride` of `1` to capture the largest amount of detail. This is the default value, so we can leave it out for now."
      ],
      "metadata": {
        "id": "e9YNNkic5M15"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Parameters for Pooling Layers\n",
        "\n",
        "There a few parameters that we have to specify for each ***pooling*** layer that we add. Below is a description of what they are and how to select appropriate values.\n",
        "\n",
        "#### **`kernel_size`: The size of the pooling kernel**\n",
        "\n",
        "* `kernel_size` refers to the size of the pooling kernel that you are applying. A common value to select in practice is `2`. The reduces the size of the feature map by an order of 2.\n",
        "\n",
        "#### **`stride`: The number of pixels we shift our kernel**\n",
        "\n",
        "* `stride` refers to the size of the shift of the pooling kernel across the image at each step.\n",
        "* Practitioners oven opt for no overlap with their pooling kernel. Hence, if you select `2` for your pooling `kernel_size`, then you should select `2` for the `stride`.\n"
      ],
      "metadata": {
        "id": "5vOPVP_bJCOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Parameters for Fully Connected Layers\n",
        "\n",
        "There a few parameters that we have to specify for our ***fully connected*** layer. We must specify 2 numbers in `nn.Linear(#, #)`.\n",
        "\n",
        "* The first number is the input depth (i.e., the `output_size` of the last layer).\n",
        "* The final number is the number of classes in your dataset."
      ],
      "metadata": {
        "id": "b2_bZyvUKV2r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Let's build your first custom CNN!"
      ],
      "metadata": {
        "id": "OYdTX1BHK5gr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN model\n",
        "class CustomCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(32 * 7 * 7, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "qqO0Ju9Bei1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Prepare the Data\n",
        "\n",
        "For this lab, we will be using the MNIST dataset. This is a popular dataset containing black-and-white images of numbers.\n",
        "\n",
        "Before we can train a neural network on images, we often need to prepare the images so the model can understand them. That’s what the `transform` section does: tells PyTorch how to process each image when it’s loaded from the dataset. Think of `Compose` as a pipeline or a to-do list for how each image should be transformed before it’s given to the model. This particular set of transforms does two things: (1) converts the image data to Tensors and (2) normalizes (i.e., centers and scales) the pixel values (0.1307 is mean and 0.3081 is standard deviation, these come from the average pixel values of the MNIST dataset).\n",
        "\n",
        "Check out the documentation for [`DataLoader` here](https://docs.pytorch.org/docs/stable/data.html) to help you create your `trainloader` and `testloader`."
      ],
      "metadata": {
        "id": "EdFe0STFK996"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = DataLoader(testset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "2O5dEFQ08tA2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46d6aaaf-cc6a-4d76-968e-5bf911cd8e1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:02<00:00, 4.59MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 126kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.19MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 13.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Training the CNN\n",
        "\n",
        "First we must specify a few things about our training process:\n",
        "* What model we will use,\n",
        "* what loss function we want,\n",
        "* and what optimizer we want (where we also specify our learning rate).\n",
        "\n",
        "Check out the documentation for different [loss functions](https://docs.pytorch.org/docs/stable/nn.html#loss-functions) and for different [optimizers](https://docs.pytorch.org/docs/stable/optim.html) to decide what to use."
      ],
      "metadata": {
        "id": "3ELW6kgALNuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# specify that we want gpu if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# send model to device\n",
        "model = CustomCNN().to(device)\n",
        "\n",
        "# specify loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "S4mNpkxm8u23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can create a loop to run through the batches of data and update the weights for each batch.\n",
        "\n",
        "We will also save the training loss and report it back once after each epoch."
      ],
      "metadata": {
        "id": "lzZbFQ_WLee8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in trainloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(trainloader):.4f}\")\n",
        "\n",
        "print(\"Training complete!\")"
      ],
      "metadata": {
        "id": "oVm9IdZH8w4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3eabbba3-116b-4403-eb1c-710a2ac1ec95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.1379\n",
            "Epoch 2, Loss: 0.0837\n",
            "Epoch 3, Loss: 0.0791\n",
            "Epoch 4, Loss: 0.0803\n",
            "Epoch 5, Loss: 0.0722\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Congratulations**! You've successfully trained your first custom CNN! Try and mess with the training hyperparameters to see if you can improve your outcomes. Be sure to look at all of the training metrics we've learned in class!"
      ],
      "metadata": {
        "id": "VMSyj5RxLpN-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Transfer Learning with ResNet18\n",
        "\n",
        "Now that you’ve built and trained your own CNN from scratch, we can move on to another powerful approach called **transfer learning**.\n",
        "\n",
        "Transfer learning lets us take a model that has already been trained on a large dataset and adapt it for a new task. This is especially useful when:\n",
        "- You have limited data for your new task\n",
        "- You want to save time and computational resources\n",
        "- You want to take advantage of the model’s ability to extract useful features that it already learned\n",
        "\n",
        "In this section, we will take a pretrained **ResNet18** model (originally trained on ImageNet) and fine-tune it to classify MNIST digits.\n"
      ],
      "metadata": {
        "id": "K99s2QvgZYlo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.1 Modifying a Pretrained Model\n",
        "\n",
        "The pretrained ResNet18 model expects color images (3 channels) of size 224x224 and produces outputs for 1000 ImageNet classes.\n",
        "\n",
        "We will modify:\n",
        "1. The first convolutional layer to accept grayscale input (1 channel).\n",
        "2. The final fully connected layer to output 10 classes (digits 0–9)."
      ],
      "metadata": {
        "id": "T2qe2dSiZnPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet18(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ResNet18, self).__init__()\n",
        "        self.resnet18 = models.resnet18(pretrained=True)\n",
        "\n",
        "        # Adjust first conv layer for grayscale (1 input channel)\n",
        "        self.resnet18.conv1 = nn.Conv2d(\n",
        "            1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
        "        )\n",
        "\n",
        "        # Adjust final layer for 10 MNIST classes\n",
        "        num_ftrs = self.resnet18.fc.in_features\n",
        "        self.resnet18.fc = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet18(x)"
      ],
      "metadata": {
        "id": "-t_Ty3N2ZcUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.2 Preparing the Data\n",
        "\n",
        "ResNet18 was trained on ImageNet images of size 224x224, so we will resize MNIST images to match that shape.\n",
        "\n",
        "We also apply normalization using the standard MNIST mean and standard deviation."
      ],
      "metadata": {
        "id": "EtC081iWZtvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
        "testloader = DataLoader(testset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "omSPlZU-ZvIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.3 Model, Loss, and Optimizer Setup\n",
        "\n",
        "We’ll use a smaller learning rate here because we are starting from pretrained weights rather than random initialization.\n"
      ],
      "metadata": {
        "id": "ljQyI1llZyyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = ResNet18().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFydF5PdZ2h_",
        "outputId": "9f5c74ac-4f92-4731-f085-c99520fe297d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 138MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.4 Training the Pretrained Model\n",
        "\n",
        "This training loop is very similar to what you did before.\n",
        "\n",
        "The key difference is that now we are fine-tuning an existing model rather than training from scratch.\n"
      ],
      "metadata": {
        "id": "4dRhrdxoZ4PH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 2\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for inputs, labels in trainloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(trainloader):.4f}\")\n",
        "\n",
        "print(\"Training complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hg_-c1smZ6AP",
        "outputId": "d9843616-0254-400f-82a6-0a1d38413d40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/2], Loss: 0.0783\n",
            "Epoch [2/2], Loss: 0.0420\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in testloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"Test Accuracy (whole model): {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1XRW5HKhFaL",
        "outputId": "0248449d-24c9-4b39-e190-0878d872c503"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy (whole model): 99.13%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.6 Comparing Models\n",
        "\n",
        "At this point, you have trained two models on the same dataset:\n",
        "\n",
        "1. Your **custom CNN**, which you built and trained from scratch.\n",
        "2. The **pretrained ResNet18**, which you fine-tuned using transfer learning.\n",
        "\n",
        "Try comparing:\n",
        "- The training time for each model\n",
        "- The test accuracy\n",
        "- How quickly each model starts to perform well\n",
        "\n",
        "You should notice that the pretrained model reaches good accuracy faster, even though it has many more parameters. This is because it already knows how to extract useful image features from its earlier training on ImageNet.\n",
        "\n",
        "This is the core idea of transfer learning: we reuse knowledge from one large task to improve performance on a smaller one.\n"
      ],
      "metadata": {
        "id": "8ojHKHbwZ8bu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.7 Optional Experiment: Freezing Early Layers\n",
        "\n",
        "When we fine-tune a pretrained model, we can choose **how much of the network to update**.\n",
        "\n",
        "Sometimes we only want to train the final layers and keep the earlier layers fixed. This can make training faster and help prevent overfitting when the dataset is small.\n",
        "\n",
        "Here, we will try freezing the feature extraction layers of ResNet18 and retraining only the classifier.\n"
      ],
      "metadata": {
        "id": "OBhH2C3raJ4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze all pretrained layers\n",
        "model_frozen = ResNet18().to(device)\n",
        "for param in model_frozen.resnet18.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace the final layer (unfrozen)\n",
        "num_ftrs = model_frozen.resnet18.fc.in_features\n",
        "model_frozen.resnet18.fc = nn.Linear(num_ftrs, 10).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_frozen.resnet18.fc.parameters(), lr=0.001)  # only train last layer\n"
      ],
      "metadata": {
        "id": "AJpe5WVQaLo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we train again, but this time only the final layer’s weights are updated."
      ],
      "metadata": {
        "id": "aCvKs1KmaPeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 2\n",
        "for epoch in range(epochs):\n",
        "    model_frozen.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for inputs, labels in trainloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_frozen(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(trainloader):.4f}\")\n",
        "\n",
        "print(\"Training complete (frozen feature extractor)!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHiJ2DhRaRVX",
        "outputId": "44ae6ae9-700d-41ad-c499-6848b3d9107d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/2], Loss: 0.3474\n",
            "Epoch [2/2], Loss: 0.1666\n",
            "Training complete (frozen feature extractor)!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.8 Evaluating the Frozen Model\n",
        "\n",
        "Let’s check how the frozen model performs compared to the fully fine-tuned one."
      ],
      "metadata": {
        "id": "pnihLencaYoo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_frozen.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in testloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model_frozen(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy_frozen = 100 * correct / total\n",
        "print(f\"Test Accuracy (frozen layers): {accuracy_frozen:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXNTt9PtaZ_n",
        "outputId": "084fd027-25f6-4bcc-e76e-7e00da872883"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy (frozen layers): 95.62%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.9 Discussion\n",
        "\n",
        "Compare the results:\n",
        "- How fast did each model train?\n",
        "- How did accuracy change when we froze the early layers?\n",
        "- Which approach seems more efficient for MNIST?\n",
        "\n",
        "You’ll likely see that even without updating all the weights, the frozen model still performs well. This is because the early layers of ResNet18 have already learned general-purpose features that transfer well to many visual tasks.\n",
        "\n",
        "This experiment highlights the flexibility of transfer learning and why it is widely used in computer vision projects today.\n"
      ],
      "metadata": {
        "id": "c1LUk2ehadhn"
      }
    }
  ]
}