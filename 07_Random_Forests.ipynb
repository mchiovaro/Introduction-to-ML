{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forests, Cross-Validation, and Grid Searching\n",
        "\n",
        "In this lab, you will learn how to implement a Random Forest classifier using Scikit-Learn, evaluate its performance with k-fold cross-validation, and optimize hyperparameters using GridSearchCV with cross-validation. This comprehensive approach allows you to understand model training, evaluation, and tuning, equipping you with the skills to build robust machine learning models."
      ],
      "metadata": {
        "id": "3J3MqvIpatO6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Implementing Random Forests\n",
        "\n",
        "In this section, you will:\n",
        "\n",
        "- **Load the Dataset**: Begin by importing the necessary libraries and loading a dataset (like the Iris dataset).\n",
        "- **Data Splitting**: Split the dataset into training and testing sets to ensure that you can evaluate model performance on unseen data.\n",
        "- **Model Initialization**: Initialize the Random Forest classifier without specifying hyperparameters initially.\n",
        "- **Model Training**: Train the Random Forest model using the training data and evaluate its performance on the test set."
      ],
      "metadata": {
        "id": "9YhzuAVnbXgy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Import Libraries**\n",
        "\n",
        "Begin by importing the necessary libraries. You will need pandas for data manipulation, scikit-learn for model implementation, and numpy for numerical operations.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "```"
      ],
      "metadata": {
        "id": "ST4ZfcrIbZon"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Load the Dataset**\n",
        "\n",
        "For this exercise, we will use the Iris dataset, which is a classic dataset for classification tasks. It contains features related to different species of iris flowers.\n",
        "\n",
        "```python\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "```"
      ],
      "metadata": {
        "id": "OReo4qE-bcAT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Split the Data**\n",
        "\n",
        "Next, split the dataset into training and testing sets. This is crucial to evaluate the model's performance on unseen data. We will use 80% of the data for training and 20% for testing. Use a `random_state` of `42`.\n",
        "\n",
        "If you can't remember how to split your data, take a look back at our previous labs!\n",
        "\n",
        "##### <font color='red'>**TRY IT**</font> &#x1f9e0;: Import libraries, load the data, and make the splits!"
      ],
      "metadata": {
        "id": "QLAAuDjCbect"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Initialize the Random Forest Classifier**\n",
        "\n",
        "Now, initialize the Random Forest Classifier. You can set the number of trees (`n_estimators`) and the random state for reproducibility.\n",
        "\n",
        "```python\n",
        "# Initialize the Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=5, random_state=42)\n",
        "```"
      ],
      "metadata": {
        "id": "3cap3fFQbiKB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Train the Model**\n",
        "\n",
        "Fit the model to the training data. This step involves training the Random Forest on the features and corresponding labels.\n",
        "\n",
        "```python\n",
        "# Train the model\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "```"
      ],
      "metadata": {
        "id": "K_Kt3OH9bkG5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6: Make Predictions**\n",
        "\n",
        "After training the model, use it to make predictions on the test set. This will help you understand how well the model performs on unseen data.\n",
        "\n",
        "```python\n",
        "# Make predictions on the test set\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "```"
      ],
      "metadata": {
        "id": "bGaalKzkbmGk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 7: Evaluate the Model**\n",
        "\n",
        "Finally, evaluate the modelâ€™s performance using accuracy and a classification report, which provides a detailed breakdown of the model's performance.\n",
        "\n",
        "```python\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "```\n",
        "\n",
        "##### <font color='red'>**TRY IT**</font> &#x1f9e0;: Initialize the `RandomForestClassifier`, train it, make predictions, and evaluate the model! Be sure to use more that just `accuracy_score` for your evaluations. Look back at previous labs if you need a refresher!"
      ],
      "metadata": {
        "id": "X-9I1amkbpkE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Implementing k-Fold Cross-Validation\n",
        "\n",
        "In this section, you will:\n",
        "\n",
        "- **Define K-Fold**: Set up a k-fold cross-validation strategy to evaluate the model's performance more reliably.\n",
        "- **Model Evaluation**: For each fold, train the model on a subset of the data and evaluate it on the remaining data.\n",
        "- **Performance Metrics**: Collect and compute the average cross-validation scores to assess model stability and robustness.\n"
      ],
      "metadata": {
        "id": "hFciaax2brih"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Import Additional Libraries**\n",
        "\n",
        "You will need `KFold` and `cross_val_score function` from Scikit-Learn to perform cross-validation.\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "```"
      ],
      "metadata": {
        "id": "hkReHGmjbwLr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Initialize the Random Forest Classifier Again**\n",
        "\n",
        "Create an instance of the Random Forest Classifier for use in cross-validation.\n",
        "\n",
        "```python\n",
        "# Initialize the Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=5, random_state=42)\n",
        "```"
      ],
      "metadata": {
        "id": "h6_Cy2j-b1MP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Perform Cross-Validation**\n",
        "\n",
        "`KFold` is a cross-validation strategy that splits the dataset into 'k' consecutive folds (or subsets).\n",
        "\n",
        "It provides an iterator that can be used to generate training and testing splits for model evaluation. You need to manually loop over these folds to train and evaluate your model.\n",
        "\n",
        "By using `KFold`, you have more control over the training and testing process, allowing you to customize what happens in each fold (e.g., applying different preprocessing steps).\n",
        "\n",
        "```python\n",
        "# Set up K-Fold Cross-Validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Collect scores from each fold\n",
        "cv_scores = []\n",
        "for train_index, test_index in kf.split(X_train):\n",
        "    X_fold_train, X_fold_test = X_train[train_index], X_train[test_index]\n",
        "    y_fold_train, y_fold_test = y_train[train_index], y_train[test_index]\n",
        "    \n",
        "    rf_classifier.fit(X_fold_train, y_fold_train)\n",
        "    y_fold_pred = rf_classifier.predict(X_fold_test)\n",
        "    score = accuracy_score(y_fold_test, y_fold_pred)\n",
        "    cv_scores.append(score)\n",
        "\n",
        "# Print the average cross-validation score\n",
        "print(\"K-Fold Cross-Validation Scores:\", cv_scores)\n",
        "print(f\"Mean Cross-Validation Accuracy: {sum(cv_scores) / len(cv_scores):.2f}\")\n",
        "```\n",
        "\n",
        "##### <font color='red'>**TRY IT**</font> &#x1f9e0;: Perform k-Fold Cross-Validation on your Random Forest. Try editing the above loop to keep track of another performance metric (e.g., precision, recall, F-1)."
      ],
      "metadata": {
        "id": "Z8Ao4EwSb4Ft"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "But guess what!? There's a method that folds together these steps for us (the splitting and the re-training for each fold).\n",
        "\n",
        "`cross_val_score` offers a simpler interface for evaluating models with cross-validation, automatically handling the splitting and scoring process. However, it does not allow you as much control.\n",
        "\n",
        "```python\n",
        "# Perform 5-fold cross-validation with 'cross_val_score'\n",
        "cv_scores = cross_val_score(rf_classifier, X, y, cv=5)\n",
        "```\n",
        "\n",
        "##### <font color='red'>**TRY IT**</font> &#x1f9e0;: Try out this simpler function. What do you notice about its output?"
      ],
      "metadata": {
        "id": "yU92_vNBdyif"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: Implementing a Grid Search\n",
        "\n",
        "In this section, you will:\n",
        "\n",
        "- **Hyperparameter Tuning**: Define a grid of hyperparameters to search over, optimizing the Random Forest model for better performance.\n",
        "- **Grid Search Setup**: Use GridSearchCV to automate the process of hyperparameter tuning with cross-validation.\n",
        "- **Model Selection**: Identify the best hyperparameters based on cross-validation scores and make predictions using the optimized model."
      ],
      "metadata": {
        "id": "GhccDV7WeinV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Define a grid of hyperparameters to search over.**\n",
        "\n",
        "Three hyperparameters are commonly tuned for a Random Forest classifier:\n",
        "\n",
        "1. `n_estimators`\n",
        "  - **Definition**: This hyperparameter specifies the number of decision trees in the Random Forest model.\n",
        "  - **Impact**: Increasing the number of estimators generally improves the model's performance and stability, as it allows for more diverse trees to contribute to the final prediction. However, it also increases computational cost and may lead to diminishing returns after a certain point.\n",
        "  - **Typical Values**: Common choices are 50, 100, and 200. The optimal value often depends on the dataset size and complexity.\n",
        "2. `max_depth`\n",
        "  - **Definition**: This hyperparameter controls the maximum depth of each decision tree in the forest.\n",
        "  - **Impact**: A deeper tree can capture more complex patterns in the data but is also more likely to overfit, especially with limited training data.Setting max_depth to None allows the trees to grow until all leaves are pure or contain fewer than the minimum samples required to split, which may lead to overfitting.\n",
        "  - **Typical Values**: You might explore values such as None, 10, 20, and 30. A lower value can help reduce overfitting.\n",
        "3. `min_samples_split`\n",
        "  - **Definition**: This hyperparameter defines the minimum number of samples required to split an internal node of a tree.\n",
        "  - **Impact**: Increasing this value can lead to more generalized trees, as it prevents the model from creating splits based on very few samples, thus reducing the likelihood of overfitting. A smaller value allows for more splits, which can help capture more complexity in the data but may also result in overfitting.\n",
        "  - **Typical Values**: Common choices are 2, 5, and 10. The best value can vary based on the dataset size and characteristics.\n",
        "\n",
        "We can specify values of these hyperparameters that we want to test and format them in a \"grid\". All combinations of these values will then be tested in cross-validation.\n",
        "\n",
        "```python\n",
        "# Import our new function\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "```\n",
        "\n",
        "##### <font color='red'>**TRY IT**</font> &#x1f9e0;: Create this grid and then create one of your own!"
      ],
      "metadata": {
        "id": "24GB564Nfz7l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Perform Grid Search with Cross-Validation**\n",
        "\n",
        "Use GridSearchCV to perform hyperparameter tuning. This will automatically use cross-validation to evaluate the combinations of hyperparameters.\n",
        "\n",
        "```python\n",
        "# Set up Grid Search with cross-validation\n",
        "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=kf, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "```"
      ],
      "metadata": {
        "id": "GXCHZXRAeyjB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Get the Best Parameters**\n",
        "\n",
        "Retrieve the best hyperparameters from the grid search.\n",
        "\n",
        "```python\n",
        "# Get the best parameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "```"
      ],
      "metadata": {
        "id": "lY0c0xK9e3BU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Make Predictions with the Best Model**\n",
        "\n",
        "Use the best estimator to make predictions on the test set.\n",
        "\n",
        "```python\n",
        "# Make predictions on the test set using the best model\n",
        "best_rf_classifier = grid_search.best_estimator_\n",
        "y_pred = best_rf_classifier.predict(X_test)\n",
        "```"
      ],
      "metadata": {
        "id": "nzYGQ0RifAz5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Evaluate the Model**\n",
        "\n",
        "Finally, evaluate the performance of the model with the best hyperparameters.\n",
        "\n",
        "```python\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "```\n",
        "\n",
        "##### <font color='red'>**TRY IT**</font> &#x1f9e0;: Perform a Grid Search both for the example grid above and for the one you made on your own. Which set of hyperparameters produced the best results?"
      ],
      "metadata": {
        "id": "A9sQBowQfCCe"
      }
    }
  ]
}