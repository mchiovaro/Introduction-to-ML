{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 10: Stochastic, Batch, and Mini-Batch Gradient\n",
        "\n",
        "In this coding lab, we will explore the implementation of different kinds of Gradient Descent: ***Stochastic***, ***Batch***, and ***Mini-Batch***. We will use the the ***same single-layer neural network*** we made in the last lab to classify observations in the Iris dataset.\n",
        "\n",
        "\n",
        "***Note***: This lab is structured like our Decision Tree lab. There won't be many \"<font color='red'>**TRY IT**</font> &#x1f9e0;s\", but rather, we will walk through this live together. If you are following along from outside of our course, try to fill in wherever you see \"XXXX\". If you get stuck, refer to the answer key that's also posted in the `Introduction-to-ML` repository."
      ],
      "metadata": {
        "id": "J9G399G40Mee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation\n",
        "\n",
        "This section of code is responsible for preparing the Iris dataset for training a neural network. We will: **Load the Data**, perform **One-Hot Encoding**, perform **Feature Normalization**, and create the **Train-Test Split**. Together, these ensure that the data is in the right format and scale for training the neural network.\n",
        "\n",
        "**Note**: The following code block is the same as that in `09_Neural_Networks.ipynb`.\n"
      ],
      "metadata": {
        "id": "5x8uQ-zyLj8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# One-Hot Encoding the target labels\n",
        "y = y.reshape(-1, 1)\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y_encoded = encoder.fit_transform(y)\n",
        "\n",
        "# Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_normalized = scaler.fit_transform(X)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y_encoded, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "4smKIuDBnIvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the Neural Network Structure\n",
        "We'll use ***the same simple one-layer neural network*** as we did in `09_Neural_Networks.ipynb` but we will add the ability to perform stochastic, batch, and mini-batch gradient descent.\n",
        "\n",
        "A few changes should be noted:\n",
        "- We have set some seeds (`np.random.seed(42)`) so that our work is reproducible.\n",
        "- We have added new parameters to the `train()` function.\n",
        "- We have added new code within the `train()` function to allow for the different kinds of gradient descent.\n",
        "\n",
        "All other code is largely the same as that in `09_Neural_Networks.ipynb`."
      ],
      "metadata": {
        "id": "bs-woLcSLqJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNeuralNetwork:\n",
        "    def __init__(self, input_size, output_size, learning_rate=0.01):\n",
        "        \"\"\"\n",
        "        Initialize the neural network with random weights and biases.\n",
        "        \"\"\"\n",
        "        # Set the seed for reproducibility\n",
        "        np.random.seed(42)  # This ensures all random processes (e.g., weight initialization) are the same each time\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weights = np.random.randn(input_size, output_size) * 0.01  # Small random initialization\n",
        "        self.bias = np.zeros(output_size)  # Initialize biases to zeros\n",
        "\n",
        "    def softmax(self, z):\n",
        "        \"\"\"Compute softmax values for each class in z.\"\"\"\n",
        "        exp_z = np.exp(z - np.max(z))  # Stability improvement (to prevent overflow)\n",
        "        return exp_z / exp_z.sum(axis=1, keepdims=True)\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"Forward pass: computes predicted class probabilities.\"\"\"\n",
        "        z = np.dot(X, self.weights) + self.bias\n",
        "        return self.softmax(z)\n",
        "\n",
        "    def compute_loss(self, y_pred, y_true):\n",
        "        \"\"\"Compute the cross-entropy loss.\"\"\"\n",
        "        m = y_true.shape[0]  # Number of samples\n",
        "        # Get the index of the true class for each sample\n",
        "        true_class_indices = np.argmax(y_true, axis=1)\n",
        "\n",
        "        # Calculate log-likelihood using the true class indices\n",
        "        log_likelihood = -np.log(y_pred[range(m), true_class_indices])\n",
        "        loss = np.sum(log_likelihood) / m  # Average loss\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def backward(self, X, y_true, y_pred, batch_size=1):\n",
        "        \"\"\"Backward pass: computes gradients.\"\"\"\n",
        "        m = y_true.shape[0]  # Number of samples\n",
        "        true_class_indices = np.argmax(y_true, axis=1)  # True class indices for each sample\n",
        "\n",
        "        # Compute gradient of loss with respect to softmax output (cross-entropy)\n",
        "        y_pred[range(m), true_class_indices] -= 1\n",
        "        dw = np.dot(X.T, y_pred)  # Gradient for weights\n",
        "        db = np.sum(y_pred, axis=0)  # Gradient for bias\n",
        "\n",
        "        # For batch or mini-batch, normalize gradients by batch size\n",
        "        dw /= batch_size  # Normalize by batch size\n",
        "        db /= batch_size  # Normalize by batch size\n",
        "\n",
        "        return dw, db\n",
        "\n",
        "    def update_parameters(self, dw, db):\n",
        "        \"\"\"Update weights and biases using computed gradients.\"\"\"\n",
        "        self.weights -= self.learning_rate * dw  # Adjust weights\n",
        "        self.bias -= self.learning_rate * db  # Adjust biases\n",
        "\n",
        "    def train(self, X, y, epochs=1000, XXXX, XXXX):\n",
        "        \"\"\"\n",
        "        Train the neural network using the provided data.\n",
        "        \"\"\"\n",
        "        loss_history = []  # To store loss values\n",
        "        m = X.shape[0]  # Number of samples\n",
        "\n",
        "        # Shuffle the data with a fixed seed before each epoch\n",
        "        np.random.seed(42)  # Ensure reproducibility in data shuffling\n",
        "        indices = np.random.permutation(m)\n",
        "        X_shuffled = X[indices]\n",
        "        y_shuffled = y[indices]\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0  # Variable to accumulate loss for this epoch\n",
        "\n",
        "            if gd_type == \"XXXX\":\n",
        "                # Forward pass for the entire dataset\n",
        "                y_pred = self.forward(X_shuffled)\n",
        "                loss = self.compute_loss(y_pred, y_shuffled)\n",
        "\n",
        "                # Backward pass and update parameters\n",
        "                dw, db = self.backward(X_shuffled, y_shuffled, y_pred, batch_size=m)\n",
        "                self.update_parameters(dw, db)\n",
        "                total_loss += loss\n",
        "\n",
        "            elif gd_type == \"stochastic\":\n",
        "                for XXXX:\n",
        "                    # get one sample\n",
        "                    X_i = XXXX\n",
        "                    y_i = XXXX\n",
        "\n",
        "                    # forward pass\n",
        "                    y_pred = self.forward(X_i)\n",
        "                    loss = self.compute_loss(y_pred, y_i)\n",
        "\n",
        "                    # backward pass (batch size of 1 for SGD)\n",
        "                    dw, db = self.backward(X_i, y_i, y_pred, batch_size=1)\n",
        "                    self.update_parameters(dw, db)\n",
        "                    total_loss += loss\n",
        "\n",
        "            elif gd_type == \"XXXX\":\n",
        "                for XXXX:\n",
        "                    # get one batch\n",
        "                    X_batch = XXXX\n",
        "                    y_batch = XXXX\n",
        "\n",
        "                    # forward pass\n",
        "                    y_pred = self.forward(X_batch)\n",
        "                    loss = self.compute_loss(y_pred, y_batch)\n",
        "\n",
        "                    # backward pass (use actual batch size)\n",
        "                    dw, db = self.backward(X_batch, y_batch, y_pred, batch_size=batch_size)\n",
        "                    self.update_parameters(dw, db)\n",
        "                    total_loss += loss\n",
        "\n",
        "            # Average loss for the epoch\n",
        "            avg_loss = XXXX / XXXX\n",
        "            loss_history.append(avg_loss)\n",
        "\n",
        "            # Print loss every few epochs for monitoring\n",
        "            if epoch % 10 == 0:\n",
        "                print(f'Epoch {epoch}, Loss: {avg_loss:.4f}')\n",
        "\n",
        "        # Plot the loss over epochs\n",
        "        plt.plot(loss_history)\n",
        "        plt.title('Loss over Epochs')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.grid()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "Jd14BoaWefEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the Neural Network\n",
        "\n",
        "Specify the number of epochs, batch size, and type of gradient descent."
      ],
      "metadata": {
        "id": "ssNrrMTvH1d6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and train the neural network\n",
        "input_size = X_train.shape[1]  # this is the number of features\n",
        "output_size = y_train.shape[1]  # this is the number of classes\n",
        "nn = SimpleNeuralNetwork(input_size, output_size, learning_rate=0.01)\n",
        "\n",
        "# Re-run training\n",
        "nn.train(X=X_train, y=y_train, epochs=1000, batch_size=10, gd_type=\"mini-batch\")"
      ],
      "metadata": {
        "id": "G3Zt9c10f5H6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Making Predictions\n",
        "After training, we can use our neural network to make predictions. We just call the `forward` function we made to run the data through the neural network. Then we take the max of the output probabilities to get the most likely class each observation belongs to."
      ],
      "metadata": {
        "id": "aBNMzluVLs5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "y_pred = nn.forward(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_test_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "accuracy = accuracy_score(y_test_classes, y_pred_classes)\n",
        "print(f'Accuracy: {accuracy:.2f}')"
      ],
      "metadata": {
        "id": "qs-6tuWxnrRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How were your results? Not as good as you'd like?**\n",
        "\n",
        "<font color='red'>**TRY IT**</font> &#x1f9e0;: Try changing the ***gradient descent type***, ***batch size***, or ***number of epochs***. Still not happy with your results? Change the ***learning rate*** and see what happens!"
      ],
      "metadata": {
        "id": "-d_eC7Ynpks0"
      }
    }
  ]
}