{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 4: Classifiers - Part 1\n",
        "\n",
        "In this notebook, we will explore two fundamental machine learning algorithms: K-Nearest Neighbors (KNN) and Logistic Regression. Both methods are widely used for classification tasks, each with its own strengths and applications."
      ],
      "metadata": {
        "id": "ri7C5G3F7mpi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: k-Nearest Neighbors\n",
        "\n",
        "k-Nearest Neighbors (k-NN) is a simple yet powerful algorithm used for classification tasks in machine learning. It operates on the principle that similar data points tend to belong to the same class.\n",
        "\n",
        "When making a prediction for a new data point, k-NN looks at the ùëò nearest data points in the feature space (based on a distance metric, such as Euclidean distance) and assigns the most common class among those neighbors to the new point.\n"
      ],
      "metadata": {
        "id": "oer_IeaHJSaS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Import Libraries\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "```\n",
        "\n",
        "##### <font color='red'>**TRY IT**</font> &#x1f9e0;: Import these libraries."
      ],
      "metadata": {
        "id": "4DRsqMP3EY-U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Prepare the Data\n",
        "In this example, we‚Äôll use the Iris dataset, which is included in Scikit-learn. We can import it with `load_iris()`. You also need to parse your variables and split your data for training.\n",
        "\n",
        "**Our question is**: Can we predict the species of an iris flower based on informatin about it's sepals and petals?\n",
        "\n",
        "The **variables** in the dataset are:\n",
        "- **Sepal Length (cm)**: The length of the sepal (numeric, continuous).\n",
        "- **Sepal Width (cm)**: The width of the sepal (numeric, continuous).\n",
        "- **Petal Length (cm)**: The length of the petal (numeric, continuous).\n",
        "- **Petal Width (cm)**: The width of the petal (numeric, continuous).\n",
        "- **Species**: The species of the iris flower, either Iris Setosa, Iris Versicolor, or Iris Virginica (categorical). We will be removing Iris Virginica to keep things binary.\n",
        "\n",
        "```python\n",
        "# load data and create dataframe\n",
        "iris = load_iris()\n",
        "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "iris_df['species'] = iris.target\n",
        "iris_df['species'] = iris_df['species'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
        "iris_df['species'] = pd.Categorical(iris_df['species'])\n",
        "```\n",
        "\n",
        "##### <font color='red'>**TRY IT**</font> &#x1f9e0;: (1) Load the data using the function given above. (2) Parse the data into two new variables: y (for the single target) and X (for the predictors). (3) Split the data into `X_train`, `X_test`, `y_train`, `y_test`, using a `test_size` of .25 and a `random_state` of 22.\n",
        "(*Refer back to out Lab 2 notebook if you need to refresh on how to do  this!*)"
      ],
      "metadata": {
        "id": "tM4cRXXTI4NJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "iris = load_iris()\n",
        "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "iris_df['species'] = iris.target\n",
        "iris_df['species'] = iris_df['species'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
        "#iris_df = iris_df[iris_df['species'] != 'virginica']\n",
        "iris_df['species'] = pd.Categorical(iris_df['species'])\n",
        "\n",
        "print(iris_df)\n",
        "X = iris_df.drop('species', axis=1)  # Features\n",
        "y = iris_df['species']  # Target variable\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=22) # Split data into training and testing sets\n",
        "\n",
        "k = 4  # number of neighbors\n",
        "knn = KNeighborsClassifier(n_neighbors=k)\n",
        "\n",
        "# Fit the model\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print('Confusion Matrix:\\n', conf_matrix)\n",
        "\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "print('Classification Report:\\n', class_report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wdxm4qQyXset",
        "outputId": "b5500c54-7988-4553-d2b0-6171df0eca00"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
            "0                  5.1               3.5                1.4               0.2   \n",
            "1                  4.9               3.0                1.4               0.2   \n",
            "2                  4.7               3.2                1.3               0.2   \n",
            "3                  4.6               3.1                1.5               0.2   \n",
            "4                  5.0               3.6                1.4               0.2   \n",
            "..                 ...               ...                ...               ...   \n",
            "145                6.7               3.0                5.2               2.3   \n",
            "146                6.3               2.5                5.0               1.9   \n",
            "147                6.5               3.0                5.2               2.0   \n",
            "148                6.2               3.4                5.4               2.3   \n",
            "149                5.9               3.0                5.1               1.8   \n",
            "\n",
            "       species  \n",
            "0       setosa  \n",
            "1       setosa  \n",
            "2       setosa  \n",
            "3       setosa  \n",
            "4       setosa  \n",
            "..         ...  \n",
            "145  virginica  \n",
            "146  virginica  \n",
            "147  virginica  \n",
            "148  virginica  \n",
            "149  virginica  \n",
            "\n",
            "[150 rows x 5 columns]\n",
            "Accuracy: 0.95\n",
            "Confusion Matrix:\n",
            " [[11  0  0]\n",
            " [ 0 13  0]\n",
            " [ 0  2 12]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        11\n",
            "  versicolor       0.87      1.00      0.93        13\n",
            "   virginica       1.00      0.86      0.92        14\n",
            "\n",
            "    accuracy                           0.95        38\n",
            "   macro avg       0.96      0.95      0.95        38\n",
            "weighted avg       0.95      0.95      0.95        38\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Create and Train the k-NN Model\n",
        "\n",
        "Instantiating the k-NN classifier and fitting it to the training data is quite easy. Let's try it out with a ùëò of 4 to start.\n",
        "\n",
        "```python\n",
        "# Create the k-NN model\n",
        "k = 4  # number of neighbors\n",
        "knn = KNeighborsClassifier(n_neighbors=k)\n",
        "\n",
        "# Fit the model\n",
        "knn.fit(X_train, y_train)\n",
        "```\n",
        "Step 5: Make Predictions\n",
        "Use the trained model to make predictions on the test set.\n",
        "\n",
        "```python\n",
        "# Make predictions\n",
        "y_pred = knn.predict(X_test)\n",
        "```\n",
        "\n",
        "##### <font color='red'>**TRY IT**</font> &#x1f9e0;: Fit the classifier and predict on your data!"
      ],
      "metadata": {
        "id": "cKm4It07I6sT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Evaluate the Model\n",
        "Evaluate the model's performance using accuracy, a confusion matrix, and a classification report.\n",
        "\n",
        "**4.1. Accuracy Score**\n",
        "\n",
        "Accuracy represents the proportion of correctly predicted instances out of the total instances. It‚Äôs a simple measure, but can be misleading if the classes are imbalanced.\n",
        "\n",
        "```python\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "```\n",
        "\n",
        "**4.2. Confusion Matrix**\n",
        "\n",
        "The confusion matrix shows the counts of true positive, true negative, false positive, and false negative predictions. It helps you understand where the model is making errors, such as confusing one class for another.\n",
        "\n",
        "```python\n",
        "# Confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print('Confusion Matrix:\\n', conf_matrix)\n",
        "```\n",
        "\n",
        "**4.3. Classification Report**\n",
        "\n",
        "The classification report includes precision, recall, and F1-score for each class:\n",
        "- **Precision**: The ratio of true positives to the sum of true positives and false positives. It indicates how many of the predicted positive instances are actually positive.\n",
        "- **Recall (Sensitivity)**: The ratio of true positives to the sum of true positives and false negatives. It shows how many actual positive instances were captured by the model.\n",
        "- **F1-Score**: The harmonic mean of precision and recall. It provides a balance between the two, particularly useful when you need to consider both false positives and false negatives.\n",
        "\n",
        "```python\n",
        "# Classification report\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "print('Classification Report:\\n', class_report)\n",
        "```\n",
        "\n",
        "**Overall Scores**\n",
        "\n",
        "We can also compute summary scores across all classes using macro or micro averaging:\n",
        "- `average='macro'`: Unweighted mean (treats all classes equally).\n",
        "- `average='micro'`: Total true positives, false positives, and false negatives are computed globally.\n",
        "\n",
        "```python\n",
        "precision = precision_score(y_test, y_pred, average='macro')\n",
        "recall = recall_score(y_test, y_pred, average='macro')\n",
        "f1 = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "print(f'Precision (Macro): {precision:.2f}')\n",
        "print(f'Recall (Macro): {recall:.2f}')\n",
        "print(f'F1 Score (Macro): {f1:.2f}')\n",
        "```\n",
        "\n",
        "##### <font color='red'>**TRY IT**</font> &#x1f9e0;: Check out Accuracy, the Confusion Matrix, the Classification Report, and the overall metrics for your model. What do you notice? Did it perform well on the test data?"
      ],
      "metadata": {
        "id": "xOZ8kcRFI_HI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Visualizing the Results\n",
        "You can visualize the confusion matrix using a heatmap. The colors in a heatmap tell you how well or poorly a model performed on each class.\n",
        "\n",
        "```python\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "```\n",
        "##### <font color='red'>**TRY IT**</font> &#x1f9e0;: Create a heatmap to further understand the Confusion Matrix."
      ],
      "metadata": {
        "id": "TOBXlbJaJB1n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6: Repeat the process: Testing ùëò\n",
        "\n",
        "You‚Äôve successfully implemented a ùëò-NN classifier! But is our value for ùëò the best one possible? Let's find out.\n",
        "\n",
        "##### <font color='red'>**TRY IT**</font> &#x1f9e0;: Try out a new k. Keep in mind the ways in which ùëò impacts the model (overfitting/underfitting). Additionally, consider challenging yourself and try *ùëò-fold cross-validation* for a more robust way of optimizing your choice of ùëò. Did things get better or worse when you changed ùëò?"
      ],
      "metadata": {
        "id": "9shNLxW3JN_e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Logistic Regression\n",
        "\n",
        "Unlike linear regression, which predicts continuous outcomes, logistic regression predicts the likelihood that a given input belongs to a specific category.\n",
        "\n",
        "There are several forms of logistic regression (*logistic regression, multiple logistic regression, multinomial logistic regression*) but they are all implemented similarly in `sklearn`.\n",
        "\n",
        "The only **parameter** that you are required to specify is `max_iter`.\n",
        "\n",
        "- **Purpose**: Logistic regression uses iterative methods (like gradient descent) to converge on the optimal solution (to find the optimal weights). `max_iter` is the maximum number of iterations the algorithm will run during the optimization process to find the best-fitting model. If the algorithm doesn't converge to a solution within the specified number of iterations, it will stop, and you might get a warning.\n",
        "- **Value**: The default value for `max_iter` is  100, but this can be changed. You might want to increase max_iter if the model is complex and requires more iterations to converge or you receive a warning saying that the optimization did not complete."
      ],
      "metadata": {
        "id": "KphA_vqrZ_aq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Change 1: Additional imports**\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "```\n",
        "\n",
        "**Change 2: Initialize and fitting the model**\n",
        "\n",
        "```python\n",
        "# Initialize and fit the logistic regression model\n",
        "log_reg = LogisticRegression(max_iter=200)\n",
        "log_reg.fit(X_train, y_train)\n",
        "```\n",
        "Then predicting is done the same, with the `predict()` function. The evaluation metrics are also calculated with the same code as above.\n",
        "\n",
        "##### <font color='red'>**TRY IT**</font> &#x1f9e0;: Fit a logistic regression model to your data! Run all of the afformentioned evaluation metrics/methods. How does your logisitic regression model compare to your best kNN classifier? Why do you think one is better than the other?"
      ],
      "metadata": {
        "id": "4aqZKPdpaeOU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate Coefficients**\n",
        "\n",
        "Now let's see how our predictors impacted our out target.\n",
        "\n",
        "```python\n",
        "# Get the coefficients\n",
        "coefficients = log_reg.coef_\n",
        "intercept = log_reg.intercept_\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "coef_df = pd.DataFrame(coefficients, columns=iris.feature_names, index=iris.target_names)\n",
        "coef_df['intercept'] = intercept\n",
        "print(coef_df)\n",
        "```\n",
        "\n",
        "##### <font color='red'>**TRY IT**</font> &#x1f9e0;: Check out the coefficients of your model. What does each coefficient mean for your model? Write it out!"
      ],
      "metadata": {
        "id": "mbQvYgomZ3-h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Repeat the process: Model Improvement\n",
        "\n",
        "You‚Äôve successfully implemented a Logistic Regression! But is our model the best fit we can get?\n",
        "\n",
        "##### <font color='red'>**TRY IT**</font> &#x1f9e0;: Try altering something about the model (e.g., taking out a predictor, increasing/decreasing the number of interations). Did things get better or worse when you make those changes?"
      ],
      "metadata": {
        "id": "JGMlxW--EkPb"
      }
    }
  ]
}
